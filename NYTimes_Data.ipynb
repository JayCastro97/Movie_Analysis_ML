{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as request\n",
    "import urllib.parse as parse\n",
    "from pprint import pprint\n",
    "import json\n",
    "import numpy as np\n",
    "import xlsxwriter\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "\n",
    "articles = []\n",
    "articles1 = []\n",
    "articles2 = []\n",
    "articles3 = []\n",
    "articles4 = []\n",
    "articles5 = []\n",
    "articles6 = []\n",
    "\n",
    "class Crawler(object):\n",
    "    def __init__(self, url, apikey, start_offset, end_offset):\n",
    "        self.url = url\n",
    "        self.apiKey = apikey\n",
    "        self.startOffset = int(start_offset)\n",
    "        self.endOffset = int(end_offset)\n",
    "\n",
    "    def start(self):\n",
    "        #articles = []\n",
    "        #articles1 = []\n",
    "        offsets = np.arange(self.startOffset, self.endOffset)\n",
    "        \n",
    "        workbook = xlsxwriter.Workbook('NYTimesMovies.xlsx')\n",
    "        worksheet = workbook.add_worksheet()\n",
    "        line = 1\n",
    "        \n",
    "        for offset in offsets:\n",
    "        \n",
    "            url = self.url + \"/reviews/all.json\"\n",
    "\n",
    "            data = {}\n",
    "            data['api-key'] = self.apiKey\n",
    "            url_values = parse.urlencode(data)\n",
    "\n",
    "            # add api key\n",
    "            url = url + \"?\" + \"offset=\" + str(offset) + \"&\" + url_values\n",
    "            with request.urlopen(url) as response:\n",
    "                content = response.read()\n",
    "                content = content.decode('utf-8')\n",
    "                content = json.loads(content)\n",
    "                for each in content['results']:\n",
    "                    if \"display_title\" in each:\n",
    "                        display_title = each['display_title']\n",
    "                    else:\n",
    "                        continue\n",
    "                    if \"mpaa_rating\" in each:\n",
    "                        mpaa_rating = each['mpaa_rating']\n",
    "                    else:\n",
    "                        mpaa_rating = None\n",
    "                    if \"byline\" in each:\n",
    "                        byline = each['byline']\n",
    "                    else:\n",
    "                        byline = None\n",
    "                    if \"headline\" in each:\n",
    "                        headline = each['headline']\n",
    "                    else:\n",
    "                        headline = None\n",
    "                    if \"summary_short\" in each:\n",
    "                        summary_short = each['summary_short']\n",
    "                    else:\n",
    "                        summary_short = None\n",
    "                    if \"publication_date\" in each:\n",
    "                        publication_date = each['publication_date']\n",
    "                    else:\n",
    "                        headline = None\n",
    "                    if \"opening_date\" in each:\n",
    "                        opening_date = each['opening_date']\n",
    "                    else:\n",
    "                        opening_date = None\n",
    "                    articles.append({\n",
    "                        'display_title': display_title, \n",
    "                        'mpaa_rating': mpaa_rating,\n",
    "                        'byline': byline,\n",
    "                        'headline': headline,\n",
    "                        'summary_short': summary_short,\n",
    "                        'publication_date': publication_date,\n",
    "                        'opening_date': opening_date,\n",
    "                    })\n",
    "                    articles1.append({\n",
    "                        display_title\n",
    "                    })\n",
    "                    articles2.append({\n",
    "                        display_title, \n",
    "                        mpaa_rating,\n",
    "                    })\n",
    "                    articles3.append({\n",
    "                        display_title, \n",
    "                        mpaa_rating,\n",
    "                        byline\n",
    "                    })\n",
    "                    articles4.append({\n",
    "                        display_title, \n",
    "                        mpaa_rating,\n",
    "                        headline\n",
    "                    })\n",
    "                    articles5.append({\n",
    "                        display_title, \n",
    "                        mpaa_rating,\n",
    "                        summary_short\n",
    "                    })\n",
    "                    articles6.append({\n",
    "                        display_title, \n",
    "                        mpaa_rating,\n",
    "                        byline,\n",
    "                        headline,\n",
    "                        summary_short\n",
    "                    })\n",
    "                    worksheet.write('A'+str(line),display_title)\n",
    "                    worksheet.write('B'+str(line),mpaa_rating)\n",
    "                    worksheet.write('C'+str(line),byline)\n",
    "                    worksheet.write('D'+str(line),headline)\n",
    "                    worksheet.write('E'+str(line),summary_short)\n",
    "                    worksheet.write('F'+str(line),publication_date)\n",
    "                    worksheet.write('G'+str(line),opening_date)\n",
    "                    line+=1\n",
    "                \n",
    "        print(len(articles1))\n",
    "        with open('newyorktimes.json', 'w') as f:\n",
    "            for each in articles:\n",
    "                json.dump(each, f)\n",
    "                f.write('\\n')\n",
    "        \n",
    "    def filter(self, path):\n",
    "        filtered_articles = []\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                article = json.loads(line)\n",
    "                if article['display_title'] is None:\n",
    "                    continue\n",
    "                else:\n",
    "                    filtered_articles.append(article)\n",
    "        \n",
    "        print(len(filtered_articles))\n",
    "        with open('newyorktimes_filtered.json', 'w') as f:\n",
    "            for each in filtered_articles:\n",
    "                json.dump(each, f)\n",
    "                f.write('\\n')\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    crawler = Crawler(\n",
    "        url = \"https://api.nytimes.com/svc/movies/v2\",\n",
    "        apikey='5105d7c0e53347269a428d1c212e7de5',\n",
    "        start_offset=0,\n",
    "        end_offset=100\n",
    "    )\n",
    "\n",
    "crawler.start()\n",
    "#crawler.filter('newyorktimes.json1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "\n",
    "model1 = Word2Vec(articles2, min_count=1)\n",
    "X1 = model1[model1.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result1 = pca.fit_transform(X1)\n",
    "pyplot.scatter(result1[:, 0], result1[:, 1])\n",
    "words1 = list(model1.wv.vocab)\n",
    "for i, word in enumerate(words1):\n",
    "    pyplot.annotate(xy=(result1[i, 0], result1[i, 1]))\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "data_sampley = Word2Vec(articles6, min_count=1)\n",
    "data_sample = data_sampley[data_sampley.wv.vocab]\n",
    "\n",
    "data_features = data_sample[:, 1:]\n",
    "data_labels = data_sample[:, 0]\n",
    "subSpace = PCA().fit_transform(data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embedding(X, y, title=None):\n",
    "    x_min = np.min(X, 0)\n",
    "    x_max = np.max(X, 0)\n",
    "    X = (X-x_min)/(x_max-x_min)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.scatter(X[:,0], X[:,1], color=plt.cm.Set1(y/10.))\n",
    "    \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embedding3d(X, y, title=None):\n",
    "    x_min = np.min(X, 0)\n",
    "    x_max = np.max(X, 0)\n",
    "    X = (X-x_min)/(x_max-x_min)\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    ax = Axes3D(plt.figure(), rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "    ax.scatter(X[:,0], X[:,1], color=plt.cm.Set1(y/10.))\n",
    "    \n",
    "    ax.w_xaxis.set_ticklabels([])\n",
    "    ax.w_yaxis.set_ticklabels([])\n",
    "    ax.w_zaxis.set_ticklabels([])\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapLabels(clust, trainlabel, numclust):\n",
    "    labels = np.zeros_like(clust)\n",
    "    for i in range(numclust):\n",
    "        mask = (clust == i)\n",
    "        labels[mask] = stats.mode(trainlabel[mask])[0]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Fit into kmeans clustering method \"\"\"\n",
    "kmeans = KMeans(n_clusters=10).fit(data_features)\n",
    "print('Shape of cluster centers:', kmeans.cluster_centers_.shape)\n",
    "print('Shape of labels created by clusters:', kmeans.labels_.shape)\n",
    "\n",
    "plot_embedding(subSpace, kmeans.labels_, 'K-Means PCA projection')\n",
    "labels = mapLabels(kmeans.labels_, data_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Fit into Hierarchical clustering method \"\"\"\n",
    "ward = AgglomerativeClustering(n_clusters=10, linkage='ward').fit(data_features)\n",
    "wlabel = ward.labels_\n",
    "print('Number of Leaves:', ward.n_leaves_)\n",
    "print('Number of Connected Components', ward.n_components_)\n",
    "\n",
    "plot_embedding(subSpace, wlabel, 'Hierarchical PCA projection')\n",
    "labels = mapLabels(wlabel, data_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=111, min_samples=10).fit(data_features)\n",
    "dlabels = db.labels_\n",
    "db1 = DBSCAN(eps=211, min_samples=12).fit(data_features)\n",
    "dlabels1 = db1.labels_\n",
    "db2 = DBSCAN(eps=111, min_samples=10).fit(data_features)\n",
    "dlabels2 = db2.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(dlabels)) - (1 if -1 in dlabels else 0)\n",
    "n_clusters_1 = len(set(dlabels1)) - (1 if -1 in dlabels1 else 0)\n",
    "n_clusters_2 = len(set(dlabels2)) - (1 if -1 in dlabels2 else 0)\n",
    "\n",
    "plot_embedding(subSpace, dlabels, 'DBSCAN PCA projection (eps=130, min_samples=10)')\n",
    "labels = mapLabels(dlabels, data_labels, n_clusters_)\n",
    "print('Number of clusters:', n_clusters_)\n",
    "plot_embedding(subSpace, dlabels1, 'DBSCAN PCA projection (eps=130, min_samples=5)')\n",
    "labels1 = mapLabels(dlabels1, data_labels, n_clusters_1)\n",
    "print('Number of clusters:', n_clusters_1)\n",
    "plot_embedding(subSpace, dlabels2, 'DBSCAN PCA projection (eps=100, min_samples=10)')\n",
    "labels2 = mapLabels(dlabels2, data_labels, n_clusters_2)\n",
    "print('Number of clusters:', n_clusters_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subSpace1 = TSNE(n_components=3).fit_transform(data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Fit into kmeans clustering method \"\"\"\n",
    "kmeans = KMeans(n_clusters=10).fit(data_features)\n",
    "print('Shape of cluster centers:', kmeans.cluster_centers_.shape)\n",
    "print('Shape of labels created by clusters:', kmeans.labels_.shape)\n",
    "\n",
    "plot_embedding3d(subSpace1, kmeans.labels_, '3D K-Means TSNE projection')\n",
    "labels = mapLabels(kmeans.labels_, data_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Fit into Hierarchical clustering method \"\"\"\n",
    "ward = AgglomerativeClustering(n_clusters=10, linkage='ward').fit(data_features)\n",
    "wlabel = ward.labels_\n",
    "print('Number of Leaves:', ward.n_leaves_)\n",
    "print('Number of Connected Components', ward.n_components_)\n",
    "\n",
    "plot_embedding3d(subSpace1, wlabel, '3D Hierarchical TSNE projection')\n",
    "labels = mapLabels(wlabel, data_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Fit into DBSCAN clustering method \"\"\"\n",
    "db = DBSCAN(eps=1300, min_samples=10).fit(data_features)\n",
    "dlabels = db.labels_\n",
    "db1 = DBSCAN(eps=1300, min_samples=5).fit(data_features)\n",
    "dlabels1 = db1.labels_\n",
    "db2 = DBSCAN(eps=1000, min_samples=10).fit(data_features)\n",
    "dlabels2 = db2.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(dlabels)) - (1 if -1 in dlabels else 0)\n",
    "n_clusters_1 = len(set(dlabels1)) - (1 if -1 in dlabels1 else 0)\n",
    "n_clusters_2 = len(set(dlabels2)) - (1 if -1 in dlabels2 else 0)\n",
    "\n",
    "plot_embedding3d(subSpace1, dlabels, '3D DBSCAN TSNE projection (eps=1300, min_samples=10)')\n",
    "labels = mapLabels(dlabels, data_labels, n_clusters_)\n",
    "print('Number of clusters:', n_clusters_)\n",
    "plot_embedding3d(subSpace1, dlabels1, '3D DBSCAN TSNE projection (eps=1300, min_samples=5)')\n",
    "labels1 = mapLabels(dlabels1, data_labels, n_clusters_1)\n",
    "print('Number of clusters:', n_clusters_1)\n",
    "plot_embedding3d(subSpace1, dlabels2, '3D DBSCAN TSNE projection (eps=1000, min_samples=10)')\n",
    "labels2 = mapLabels(dlabels2, data_labels, n_clusters_2)\n",
    "print('Number of clusters:', n_clusters_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "X = data_features\n",
    "y = data_labels\n",
    "clf = tree.DecisionTreeRegressor()\n",
    "clf = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz \n",
    "dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"iris\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    crawler = Crawler(\n",
    "        url = \"https://api.nytimes.com/svc/movies/v2\",\n",
    "        apikey='5105d7c0e53347269a428d1c212e7de5',\n",
    "        start_offset=101,\n",
    "        end_offset=101\n",
    "    )\n",
    "\n",
    "crawler.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sampley = Word2Vec(articles6, min_count=1)\n",
    "data_sample = data_sampley[data_sampley.wv.vocab]\n",
    "\n",
    "data_features = data_sample[:, 1:]\n",
    "data_labels = data_sample[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict([data_features[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "subSpace2 = TSNE().fit_transform(data_features)\n",
    "\n",
    "X = subSpace2[:,0]\n",
    "y = subSpace2[:,1]\n",
    "\n",
    "regr_1 = DecisionTreeRegressor(max_depth=2)\n",
    "regr_2 = DecisionTreeRegressor(max_depth=5)\n",
    "regr_1.fit(X, y)\n",
    "regr_2.fit(X, y)\n",
    "\n",
    "\n",
    "#X_test = np.arange([data_features[0]])[:, np.newaxis]\n",
    "#y_1 = regr_1.predict(X_test)\n",
    "#y_2 = regr_2.predict(X_test)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, y, s=20, edgecolor=\"black\",c=\"darkorange\", label=\"data\")\n",
    "#plt.plot(X_test, y_1, color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=2)\n",
    "#plt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=2)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.unique(X), np.poly1d(np.polyfit(X, y, 1))(np.unique(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
