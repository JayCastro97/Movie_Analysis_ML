{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980\n"
     ]
    }
   ],
   "source": [
    "import urllib.request as request\n",
    "import urllib.parse as parse\n",
    "from pprint import pprint\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Crawler(object):\n",
    "    def __init__(self, url, apikey, start_offset, end_offset):\n",
    "        self.url = url\n",
    "        self.apiKey = apikey\n",
    "        self.startOffset = int(start_offset)\n",
    "        self.endOffset = int(end_offset)\n",
    "\n",
    "    def start(self):\n",
    "        articles = []\n",
    "        offsets = np.arange(self.startOffset, self.endOffset)\n",
    "        \n",
    "        for offset in offsets:\n",
    "        \n",
    "            url = self.url + \"/reviews/all.json\"\n",
    "\n",
    "            data = {}\n",
    "            data['api-key'] = self.apiKey\n",
    "            url_values = parse.urlencode(data)\n",
    "\n",
    "            # add api key\n",
    "            url = url + \"?\" + \"offset=\" + str(offset) + \"&\" + url_values\n",
    "            with request.urlopen(url) as response:\n",
    "                content = response.read()\n",
    "                content = content.decode('utf-8')\n",
    "                content = json.loads(content)\n",
    "                for each in content['results']:\n",
    "                    if \"display_title\" in each:\n",
    "                        display_title = each['display_title']\n",
    "                    else:\n",
    "                        continue\n",
    "                    if \"mpaa_rating\" in each:\n",
    "                        mpaa_rating = each['mpaa_rating']\n",
    "                    else:\n",
    "                        mpaa_rating = None\n",
    "                    if \"byline\" in each:\n",
    "                        byline = each['byline']\n",
    "                    else:\n",
    "                        byline = None\n",
    "                    if \"headline\" in each:\n",
    "                        headline = each['headline']\n",
    "                    else:\n",
    "                        headline = None\n",
    "                    if \"summary_short\" in each:\n",
    "                        summary_short = each['summary_short']\n",
    "                    else:\n",
    "                        summary_short = None\n",
    "                    if \"publication_date\" in each:\n",
    "                        publication_date = each['publication_date']\n",
    "                    else:\n",
    "                        headline = None\n",
    "                    if \"opening_date\" in each:\n",
    "                        opening_date = each['opening_date']\n",
    "                    else:\n",
    "                        opening_date = None\n",
    "                    articles.append({\n",
    "                        'display_title': display_title, \n",
    "                        'mpaa_rating': mpaa_rating,\n",
    "                        'byline': byline,\n",
    "                        'headline': headline,\n",
    "                        'summary_short': summary_short,\n",
    "                        'publication_date': publication_date,\n",
    "                        'opening_date': opening_date,\n",
    "                    })\n",
    "                \n",
    "        print(len(articles))\n",
    "        with open('newyorktimes.json', 'w') as f:\n",
    "            for each in articles:\n",
    "                json.dump(each, f)\n",
    "                f.write('\\n')\n",
    "        \n",
    "    def filter(self, path):\n",
    "        filtered_articles = []\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                article = json.loads(line)\n",
    "                if article['display_title'] is None:\n",
    "                    continue\n",
    "                else:\n",
    "                    filtered_articles.append(article)\n",
    "        \n",
    "        print(len(filtered_articles))\n",
    "        with open('newyorktimes_filtered.jsonl', 'w') as f:\n",
    "            for each in filtered_articles:\n",
    "                json.dump(each, f)\n",
    "                f.write('\\n')\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    crawler = Crawler(\n",
    "        url = \"https://api.nytimes.com/svc/movies/v2\",\n",
    "        apikey='5105d7c0e53347269a428d1c212e7de5',\n",
    "        start_offset=1,\n",
    "        end_offset=100\n",
    "    )\n",
    "\n",
    "crawler.start()\n",
    "#crawler.filter('newyorktimes.json1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
