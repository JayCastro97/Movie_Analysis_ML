{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '{'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7b02812133e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mt2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mText2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'newyorktimes.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     t2v.convert2vec(\n\u001b[1;32m    168\u001b[0m         \u001b[0;34m\"newyorktimes.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-7b02812133e2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, wordvec_path, preload)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordvec_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__read_wv__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStanfordCoreNLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://localhost:9000'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-7b02812133e2>\u001b[0m in \u001b[0;36m__read_wv__\u001b[0;34m(self, sep, preload)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_keywords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '{'."
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from pprint import pprint\n",
    "import pickle\n",
    "import json\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from randomcolor import RandomColor\n",
    "import os\n",
    "\n",
    "\n",
    "class Text2Vec(object):\n",
    "    def __init__(self, wordvec_path, preload=False):\n",
    "        self.wp = wordvec_path\n",
    "        self.wv = {}    \n",
    "        self.__read_wv__(preload=preload)\n",
    "        self.nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "\n",
    "    def __read_wv__(self, sep=\" \", preload=False):\n",
    "        if not preload:\n",
    "            with open(self.wp, 'r') as f:\n",
    "                for line in f:\n",
    "                    tmp = line.split(sep) \n",
    "                    word = tmp[0]\n",
    "                    vec = np.array([float(each) for each in tmp[1:]])\n",
    "                    self.wv[word] = vec\n",
    "            print(\"Number of tokens: \", len(self.wv))\n",
    "            # pprint(self.wv.keys())\n",
    "            # dump wordvector\n",
    "            with open('gloveWordVector.bin', 'wb') as f2:\n",
    "                pickle.dump(self.wv, f2)\n",
    "        else:\n",
    "            with open(self.wp, 'rb') as f:\n",
    "                self.wv = pickle.load(f)\n",
    "    \n",
    "    def convert2vec(self, in_path, out_path, has_keywords=False):\n",
    "        vectors = []\n",
    "        labels = []\n",
    "        with open(in_path) as f:\n",
    "            for i, line in enumerate(f):\n",
    "                data = json.loads(line)\n",
    "                display_title = data['display_title']\n",
    "                mpaa_rating = data['mpaa_rating']\n",
    "                byline = data['byline']\n",
    "                headline = data['headline']\n",
    "                opening_date = data['opening_date']\n",
    "                summary_short = data['summary_short']#...replace lead paragraph\n",
    "                tokens = []\n",
    "                try:\n",
    "                    out = self.nlp.annotate(summary_short, properties={\n",
    "                        'annotators': 'tokenize, ssplit, pos',\n",
    "                        'outputFormat': 'json'\n",
    "                    })\n",
    "                    # take sentence 1\n",
    "                    if isinstance(out, dict) and out['sentences']:\n",
    "                        sentence = out['sentences'][0]\n",
    "                        for each in sentence['tokens']:\n",
    "                            word = each['word'].lower()\n",
    "                            word = word.strip('.')\n",
    "                            word = word.strip(',')\n",
    "                            word = word.strip(')')\n",
    "                            word = word.strip('(')\n",
    "                            pos = each['pos']\n",
    "                            if \"JJ\" in pos or \"NN\" in pos or \"VB\" in pos:\n",
    "                                tokens.append(word)\n",
    "                except AssertionError:\n",
    "                    pass\n",
    "                # add headline\n",
    "                if headline:\n",
    "                    tmp = headline.split(' ')\n",
    "                    tmp = [each.strip(',').lower() for each in tmp]\n",
    "                    tmp = [each.strip('.').lower() for each in tmp]\n",
    "                    tmp = [each.strip(')').lower() for each in tmp]\n",
    "                    tmp = [each.strip('(').lower() for each in tmp]\n",
    "                    tokens += tmp\n",
    "\n",
    "                wv = None\n",
    "                fail2find = []\n",
    "                count = 0\n",
    "                for t in tokens:\n",
    "                    if t in self.wv:\n",
    "                        if wv is None:\n",
    "                            if float('inf') not in self.wv[t] and -float('inf') not in self.wv[t] and all(self.wv[t] < 1e5):\n",
    "                                wv = self.wv[t]\n",
    "                                count += 1\n",
    "                        else:\n",
    "                            if float('inf') not in self.wv[t] and -float('inf') not in self.wv[t] and all(self.wv[t] < 1e5):\n",
    "                                wv += self.wv[t]\n",
    "                                count += 1\n",
    "                    else:\n",
    "                        fail2find.append(t) \n",
    "                print(\"article %s -- Tokens not in word vector dictionary: %s\" % (i, fail2find))\n",
    "                if wv is not None:\n",
    "                    vectors.append(wv/count)\n",
    "                    labels.append(section)\n",
    "                \n",
    "        vectors = np.array(vectors)\n",
    "        print(vectors.shape)\n",
    "        print(vectors)\n",
    "        unique_labels = set(labels)\n",
    "        label_mapping = {}\n",
    "        for i, each in enumerate(unique_labels):\n",
    "            label_mapping[each] = i\n",
    "        new_labels = []\n",
    "        for each in labels:\n",
    "            new_labels.append(label_mapping[each])\n",
    "        new_labels = np.array(new_labels).reshape(-1, 1)\n",
    "        print(new_labels.shape)\n",
    "        complete_data = np.concatenate((vectors, new_labels), axis=1)\n",
    "        print(complete_data)\n",
    "        np.savetxt('NewYorkTime.csv', complete_data, delimiter=',')\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_data(data):\n",
    "        num_sample=5000\n",
    "        label = data[:, -1]\n",
    "        feature = data[:, :-1]\n",
    "\n",
    "        assignment = {}\n",
    "\n",
    "        for i in range(len(feature)):\n",
    "            if label[i] not in assignment:\n",
    "                assignment[label[i]] = []\n",
    "            \n",
    "            assignment[label[i]].append(i)\n",
    "        \n",
    "        # down sample\n",
    "        old_assignment = assignment\n",
    "        assignment = {}\n",
    "\n",
    "        indicies = []\n",
    "        for label in old_assignment:\n",
    "            last_length = len(indicies)\n",
    "            indicies += np.random.choice(\n",
    "                old_assignment[label], \n",
    "                size=min(int(num_sample/len(old_assignment)), \n",
    "                len(old_assignment[label])), replace=False\n",
    "                ).tolist()\n",
    "            assignment[label] = np.arange(last_length, len(indicies))\n",
    "        \n",
    "        feature = feature[indicies]\n",
    "        print(feature.shape) \n",
    "        print(len(indicies))\n",
    "        print(len(np.unique(indicies)))\n",
    "\n",
    "        tsne = TSNE()\n",
    "        x = tsne.fit_transform(feature)\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        # ax.plot(x[:, 0], x[:, 1], '*')\n",
    "        r = RandomColor()\n",
    "        colors = r.generate(count=len(assignment))\n",
    "        for i, label in enumerate(assignment):\n",
    "            ax.plot(x[assignment[label]][:, 0], x[assignment[label]][:, 1], '*', color=colors[i], label=label)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    t2v = Text2Vec('gloveWordVector.bin', preload=True)\n",
    "    t2v.convert2vec(\n",
    "        \"newyorktimes.json\",\n",
    "        \"NewYorkTimes.csv\",\n",
    "        has_keywords=False\n",
    "    )\n",
    "    data = pd.read_csv('NewYorkTime.csv').values\n",
    "\n",
    "    # Text2Vec.plot_data(data)\n",
    "    #guad = Guardian('guardian', 'guadianMetaData.jsonl', 'gloveWordVector.bin', preload=True)\n",
    "    #guad.convert()\n",
    "\n",
    "    #data = pd.read_csv('Guardian.csv').values\n",
    "\n",
    "    #Text2Vec.plot_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
