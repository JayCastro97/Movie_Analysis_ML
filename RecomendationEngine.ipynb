{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "import urllib.request as request\n",
    "import urllib.parse as parse\n",
    "from pprint import pprint\n",
    "import json\n",
    "import numpy as np\n",
    "import xlsxwriter\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "\n",
    "movie_titles = []\n",
    "articles = []\n",
    "\n",
    "class Crawler(object):\n",
    "    def __init__(self, url, apikey, start_offset, end_offset):\n",
    "        self.url = url\n",
    "        self.apiKey = apikey\n",
    "        self.startOffset = int(start_offset)\n",
    "        self.endOffset = int(end_offset)\n",
    "\n",
    "    def start(self):\n",
    "        #articles = []\n",
    "        #articles1 = []\n",
    "        offsets = np.arange(self.startOffset, self.endOffset)\n",
    "        \n",
    "        workbook = xlsxwriter.Workbook('NYTimesMovies.xlsx')\n",
    "        worksheet = workbook.add_worksheet()\n",
    "        line = 1\n",
    "        \n",
    "        for offset in offsets:\n",
    "        \n",
    "            url = self.url + \"/reviews/all.json\"\n",
    "\n",
    "            data = {}\n",
    "            data['api-key'] = self.apiKey\n",
    "            url_values = parse.urlencode(data)\n",
    "\n",
    "            # add api key\n",
    "            url = url + \"?\" + \"offset=\" + str(20*offset) + \"&\" + url_values\n",
    "            with request.urlopen(url) as response:\n",
    "                content = response.read()\n",
    "                content = content.decode('utf-8')\n",
    "                content = json.loads(content)\n",
    "                for each in content['results']:\n",
    "                    if \"display_title\" in each:\n",
    "                        display_title = each['display_title']\n",
    "                    else:\n",
    "                        continue\n",
    "                    movie_titles.append({\n",
    "                        display_title\n",
    "                    })\n",
    "                \n",
    "        print(len(movie_titles))\n",
    "        with open('movie_titles.json', 'w') as f:\n",
    "            for each in articles:\n",
    "                json.dump(each, f)\n",
    "                f.write('\\n')\n",
    "        \n",
    "    def filter(self, path):\n",
    "        filtered_articles = []\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                article = json.loads(line)\n",
    "                if article['display_title'] is None:\n",
    "                    continue\n",
    "                else:\n",
    "                    filtered_articles.append(article)\n",
    "        \n",
    "        print(len(filtered_articles))\n",
    "        with open('newyorktimes_filtered.json', 'w') as f:\n",
    "            for each in filtered_articles:\n",
    "                json.dump(each, f)\n",
    "                f.write('\\n')\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    crawler = Crawler(\n",
    "        url = \"https://api.nytimes.com/svc/movies/v2\",\n",
    "        apikey='5105d7c0e53347269a428d1c212e7de5',\n",
    "        start_offset=0,\n",
    "        end_offset=50\n",
    "    )\n",
    "\n",
    "crawler.start()\n",
    "#crawler.filter('newyorktimes.json1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4623673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "998"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "data_sampley = Word2Vec(movie_titles, size=1, min_count=1)\n",
    "data_sample = data_sampley[data_sampley.wv.vocab]\n",
    "data_sample2 = set(data_sample.flatten())\n",
    "data_sample3 = frozenset(data_sample2)\n",
    "data_sample4 = data_sample.tolist()\n",
    "\n",
    "movie_data = []\n",
    "print(data_sample[0][0])\n",
    "#for x in range(1000):\n",
    "#    movie_data.append({\n",
    "#        movie_titles[x],\n",
    "#        data_sample3[x][0]\n",
    "#    })\n",
    "len(data_sample4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "user_ratings = [ ( randint(1, 20), k, randint(1, 10) ) for k in range(50,951) ]\n",
    "ur_names = ['user_id','movie_id','rating']\n",
    "ratings = pd.DataFrame.from_records(user_ratings,columns=ur_names)\n",
    "user_profiles = [ ( k, randint(1, 2), randint(18, 22) ) for k in range(1,21) ]\n",
    "up_names = ['user_id','sex','age']\n",
    "users = pd.DataFrame.from_records(user_profiles,columns=up_names)\n",
    "articles6 = [ ( k, movie_titles[k], data_sample4[k][0] ) for k in range(998) ]\n",
    "movie_names = ['movie_id','movie_name','movie_data']\n",
    "items = pd.DataFrame.from_records(articles6,columns=movie_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train = ratings\n",
    "user_ratings2 = [ ( randint(1, 20), k, randint(1, 10) ) for k in range(1,50) ]\n",
    "ratings_test = pd.DataFrame.from_records(user_ratings2,columns=ur_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((901, 3), (49, 3))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_train.shape, ratings_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = 20\n",
    "n_items = 1000\n",
    "data_matrix = np.zeros((n_users, n_items))\n",
    "for line in ratings.itertuples():\n",
    "    data_matrix[line[1]-1,line[2]-1] = line[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances \n",
    "user_similarity = pairwise_distances(data_matrix, metric='cosine')\n",
    "item_similarity = pairwise_distances(data_matrix.T, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(ratings, similarity, type='user'):\n",
    "    if type == 'user':\n",
    "        mean_user_rating = ratings.mean(axis=1)\n",
    "        ratings_diff = (ratings - mean_user_rating[:, np.newaxis])\n",
    "        pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    elif type == 'item':\n",
    "        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prediction = predict(data_matrix, user_similarity, type='user')\n",
    "item_prediction = predict(data_matrix, item_similarity, type='item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 9 is a 19 year old female and has a 3.5684210526315763 percent chance of liking {'The House That Jack Built'} according to model 1\n",
      "User 9 is a 19 year old female and has a 28.52852852852853 percent chance of liking {'The House That Jack Built'} according to model 2\n"
     ]
    }
   ],
   "source": [
    "user_num = 9\n",
    "movie_num = 4\n",
    "gender = 'male'\n",
    "if (user_profiles[user_num - 1][1] == 1):\n",
    "    gender = 'male'\n",
    "else:\n",
    "    gender = 'female'\n",
    "print('User ' + str(user_num) + \" is a \" + str(user_profiles[user_num - 1][2]) + \" year old \" + gender + \" and has a \" + str(user_prediction[user_num-1][movie_num] * 100) + ' percent chance of liking ' + str(movie_titles[movie_num]) + ' according to model 1')\n",
    "print('User ' + str(user_num) + \" is a \" + str(user_profiles[user_num - 1][2]) + \" year old \" + gender + \" and has a \" + str(item_prediction[user_num-1][movie_num] * 100) + ' percent chance of liking ' + str(movie_titles[movie_num]) + ' according to model 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
